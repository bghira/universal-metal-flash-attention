#!/usr/bin/env python3
"""
Basic Universal Metal Flash Attention Python Example

Demonstrates zero-copy, high-performance attention computation using the
Universal Metal Flash Attention Python bindings.
"""

import sys
from pathlib import Path

import numpy as np

# Add the source directory to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

import umfa


def main():
    """Run basic attention example."""
    print("Universal Metal Flash Attention - Python Example")
    print("=" * 50)

    # Check system compatibility
    if not umfa.is_metal_available():
        print("âŒ Metal is not available on this device")
        return 1

    print("âœ… Metal device is supported")

    # Print version info
    major, minor, patch = umfa.get_version()
    print(f"âœ… MFA version: {major}.{minor}.{patch}")

    # Create test data
    seq_len = 512
    head_dim = 64
    print(f"âœ… Creating test tensors: seq_len={seq_len}, head_dim={head_dim}")

    # Use FP16 for optimal performance on Apple Silicon
    q = np.random.randn(seq_len, head_dim).astype(np.float16)
    k = np.random.randn(seq_len, head_dim).astype(np.float16)
    v = np.random.randn(seq_len, head_dim).astype(np.float16)

    print(f"âœ… Input shapes: Q={q.shape}, K={k.shape}, V={v.shape}")
    print(f"âœ… Data type: {q.dtype}")

    try:
        # Method 1: Using context manager (recommended)
        print("\nğŸš€ Running attention with context manager...")
        with umfa.MFAContext() as ctx:
            output1 = umfa.flash_attention_forward(
                ctx, q, k, v, causal=False, input_precision="fp16"
            )

        print(f"âœ… Output shape: {output1.shape}, dtype: {output1.dtype}")
        print(f"âœ… Output range: [{output1.min():.4f}, {output1.max():.4f}]")

        # Method 2: Using convenience function
        print("\nğŸš€ Running attention with convenience function...")
        output2 = umfa.attention(q, k, v, causal=True)

        print(f"âœ… Causal output shape: {output2.shape}, dtype: {output2.dtype}")
        print(f"âœ… Causal output range: [{output2.min():.4f}, {output2.max():.4f}]")

        # Verify outputs are different (causal vs non-causal)
        diff = np.mean(np.abs(output1 - output2))
        print(f"âœ… Difference between causal/non-causal: {diff:.6f}")

        # Test different precisions
        print("\nğŸ§ª Testing different precisions...")

        # FP32 precision
        q_fp32 = q.astype(np.float32)
        k_fp32 = k.astype(np.float32)
        v_fp32 = v.astype(np.float32)

        output_fp32 = umfa.attention(
            q_fp32, k_fp32, v_fp32, input_precision="fp32", output_precision="fp32"
        )
        print(f"âœ… FP32 output: shape={output_fp32.shape}, dtype={output_fp32.dtype}")

    except umfa.MFAError as e:
        print(f"âŒ MFA Error: {e}")
        return 1
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        return 1

    print("\nâœ… All tests completed successfully!")
    print("\nğŸ“Š Performance Notes:")
    print("   â€¢ Zero-copy operation: no data copying between Python and Metal")
    print("   â€¢ FP16 precision provides optimal speed/memory tradeoffs")
    print("   â€¢ Causal masking supported for autoregressive models")
    print("   â€¢ Maintains 4400+ GINSTRS/sec MFA performance")

    return 0


if __name__ == "__main__":
    exit(main())
